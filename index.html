<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Split-and-Conquer: Client-Decoupled Split Federated Learning in Heterogeneous Environments</title>
  <style>
  body{
    font-family:system-ui,Arial;
    max-width:900px;
    margin:40px auto;
    padding:0 16px;
    line-height:1.6
  }

  h1{
    text-align: center;      /* 가운데 정렬 */
    font-size: 2.8rem;      /* 제목 크게 */
    margin-top: 0.5em;
    margin-bottom: 0.3em;
  }

  h2{
    margin-top:1.5em;
  }

  .links a{margin-right:12px}
  .card{background:#f6f7f9;border-radius:12px;padding:16px;margin:20px 0}
  img{max-width:100%;height:auto}
</style>

</head>
<body>
  <h1>Split-and-Conquer: Client-Decoupled Split Federated Learning in Heterogeneous Environments</h1>
  <p style="text-align:center;"><strong>Anonymous submission</strong></p>


  <div class="links">
    <a href="#">Paper</a>
    <a href="#">Code</a>
    <a href="#">Video</a>
  </div>

  <div class="card">
    <h2>Abstract</h2>
    <p>
      Split Federated Learning (SFL) enables collaborative training of large-scale models under privacy and resource constraints. However, existing SFL systems enforce the use of the same client-side layers, which prevents clients from personalizing the models to client-specific environments. Moreover, SFL suffers from the straggler problem, since client updates are performed sequentially across clients by receiving gradients from the server, so slow clients delay the updates of other clients. To address these issues, we propose a novel SFL framework, \textit{Split-and-Conquer}, that divides the entangled challenges into two subproblems and addresses them. We introduce a shared head and a replay mechanism that allow client-side models to be updated independently from server-side training while supporting client-specific personalization. By using a shared head located after the client-side layers to compute the loss locally, each client can personalize and update its model without server-side backpropagation. A replay buffer on the server side stores the intermediate activations from clients and enables robust server-side training even in the presence of stragglers. We further provide a convergence analysis and validate the effectiveness of our approach through extensive simulations across diverse tasks and model architectures.
    </p>
  </div>

  <div class="card">
    <h2>Method</h2>
    <p>
      Brief description of your method here.
    </p>
  </div>

  <div class="card">
    <h2>Experiments</h2>
    <p>
      Add key experimental results and figures here.
    </p>
  </div>

  <div class="card">
    <h2>BibTeX</h2>
    <pre>
@inproceedings{anonymous2026split,
  title={Split-and-Conquer},
  author={Anonymous Authors},
  year={2026}
}
    </pre>
  </div>
</body>
</html>
